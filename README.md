# Fineâ€‘TunedLLaMa

This repository is a practice project for fineâ€‘tuning theâ€¯LLaMAâ€‘2 (7B) model using parameterâ€‘efficient fineâ€‘tuning (PEFT) via LoRA, trained on the databricks/dollyâ€‘15k dataset. The goal is to demonstrate loss convergence and observable performance uplift on benchmarks such as AlpacaEval2 and MTâ€‘Bench compared to the base model.

---

## ğŸš€ Highlights

- Uses PEFT (LoRA) to fineâ€‘tune LLaMAâ€‘2â€‘7B, enabling efficient adaptation.  
- Trained on the Dollyâ€‘15K dataset from Databricks.  
- Codebase includes training scripts, dataset handling, model architecture adjustments, LoRA merging, and evaluation scaffolding.  
- Shows quantitative improvements (loss convergence and benchmark scores) over the base model.

---

## ğŸ“ Repository Structure

- `config.py` â€“ Configuration settings for model, training, dataset paths, hyperparameters.  
- `dataset.py` â€“ Handles the loading, processing, and batching of the Dollyâ€‘15K dataset.  
- `model.py` â€“ Defines the model architecture modifications (e.g., adding LoRA layers) and how the base model is wrapped.  
- `train.py` â€“ Training script (single GPU / default) for fineâ€‘tuning.  
- `train_ddp.py` â€“ Training script supporting distributed dataâ€‘parallel (DDP) training for multiple GPUs.  
- `merge_lora.py` â€“ Utility to merge the LoRA weights back into model (for inference or deployment).  
- `requirements.txt` â€“ Python dependencies for the project.  

---

## ğŸ”§ Setup & Usage

### Prerequisites

- Pythonâ€¯3.8+ (or compatible version).  
- GPUs with sufficient memory (fineâ€‘tuning LLaMAâ€‘2â€‘7B typically requires >â€¯20â€¯GB VRAM per GPU, depending on batch size).  
- Access to the base model weights for LLaMAâ€‘2â€‘7B from Meta (ensure you comply with licensing).  
- The Dollyâ€‘15K dataset (via HuggingFace or Databricks).  

### Installation
```bash
git clone https://github.com/ChaoChienHung/Fineâ€‘TunedLLaMa.git  
cd Fineâ€‘TunedLLaMa  
pip install -r requirements.txt  
```
### Configuration

Edit `config.py` to set:  

- Paths to model checkpoint, dataset files  
- LoRA hyperparameters (rank, alpha, dropout)  
- Training hyperparameters (batch size, learning rate, epochs)  
- DDP settings if using `train_ddp.py`  

### Running Training

Single GPU:
```bash
python train.py
```

Multiâ€‘GPU (DDP):
```bash
python train_ddp.py --num_gpus N
```
### After Training

You can merge the LoRA weights into the base model for inference using:
```bash
python -m utils.merge_lora                                              
```

Then use the merged model for inference or deployment.

---

## ğŸ“Š Results & Benchmarking

The fineâ€‘tuned model shows:

- Loss convergence compared to base model (plots/logs included).  
- Improved scores on AlpacaEval2 and MTâ€‘Bench vs the base LLaMAâ€‘2â€‘7B model.  
- Demonstrates that parameterâ€‘efficient fineâ€‘tuning (LoRA) works effectively for large language models.

---

## ğŸ§  Why This Matters

Fineâ€tuning large language models from scratch is expensive. Using PEFT approaches like LoRA enables:

- Much smaller number of trainable parameters (faster training, less memory).  
- Retaining the majority of the base model while adapting to a new dataset (like Dollyâ€‘15K).  
- A practical workflow to bring baseâ€‘model capabilities into a more specialized setting with fewer resources.

---

## âœ… Contribution & Usage Notes

- This is a **practice** project: meant for experimentation / learning.  
- If you adapt or extend this work, please ensure license compliance for LLaMAâ€‘2 and Dollyâ€‘15K.  
- Contributions, issues, enhancements welcome (e.g., alternative datasets, hyperparameter tuning, inference pipelines).  
- For deployment, further steps may be needed (quantization, optimization, serving infrastructure).

---

## ğŸ“š References

- Metaâ€™s LLaMAâ€‘2 model (7B variant) â€“ check licensing.  
- Databricks Dollyâ€‘15K dataset.  
- PEFT / LoRA methods for efficient fineâ€‘tuning.  
- Benchmarks: AlpacaEval2, MTâ€‘Bench.

---

## ğŸ“„ License

Specify license here (e.g., MIT, Apacheâ€¯2.0) depending on your preference.  
If no license file included, consider adding one to clarify usage rights.

---

Thank you for exploring this repository! If you have questions or suggestions, feel free to open an issue or pull request.